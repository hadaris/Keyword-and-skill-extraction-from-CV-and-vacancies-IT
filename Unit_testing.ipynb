{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/ydZ9GQPw+VXjZ0mEw0Ky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HadarMiriamIsaacson/BS-SE-24-207/blob/main/Unit_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvHsjN47OMnY",
        "outputId": "be3c9692-ebc4-467d-c5c9-2ac44f144395"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unittest\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, PreTrainedModel, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "jlzJjCp9NX5f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjTLi2XRYHzT",
        "outputId": "d405fb98-83ac-45be-8b68-e59dced44756"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score"
      ],
      "metadata": {
        "id": "245dMKyqYQF6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkoFAD5yNaJo",
        "outputId": "6cc432aa-3ce3-4963-ae21-e9173c74416a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path as a global constant\n",
        "FILE_PATH = '/content/drive/MyDrive/פרויקט גמר ניסן והדר/new/data_holy.xlsx'\n",
        "\n",
        "# Other constants for the model saving/loading\n",
        "DRIVE_SAVE_DIRECTORY = '/content/drive/MyDrive/fine-tuned-keyword-extraction-230824-test2'\n",
        "MODEL_CHECKPOINT = \"distilroberta-base\"\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, add_prefix_space=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yQ9TPDoNdCK",
        "outputId": "86064cdf-e398-4d63-eb15-e84c643d693c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Data Loading"
      ],
      "metadata": {
        "id": "wm-gSQ1GWaQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_data_loading():\n",
        "    try:\n",
        "        data = pd.read_excel(FILE_PATH)\n",
        "        assert not data.empty, \"Data should not be empty\"\n",
        "        # Update the column name to match your dataset\n",
        "        assert 'job_description' in data.columns, \"Expected 'job_description' in the data\"\n",
        "        print(\"test_data_loading: PASSED\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"test_data_loading: FAILED ({e})\")\n",
        "\n",
        "# Run the test\n",
        "test_data_loading()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33cYQSmQWUVw",
        "outputId": "fb3f5a9a-19be-472f-97c6-543588a7b375"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_data_loading: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(FILE_PATH)"
      ],
      "metadata": {
        "id": "M5uiMoaosY9B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fgO_gFNSsdbE",
        "outputId": "7a276b24-6209-41c5-8069-e682b292f574"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   job_num                                    job_description    word  hadar  \\\n",
              "0        0  The chosen Sr. Software Developer will be part...  chosen      0   \n",
              "1        0  The chosen Sr. Software Developer will be part...      sr      0   \n",
              "2        0  The chosen Sr. Software Developer will be part...    part      0   \n",
              "3        0  The chosen Sr. Software Developer will be part...  larger      0   \n",
              "4        0  The chosen Sr. Software Developer will be part...    team      0   \n",
              "\n",
              "   nisan  judge  final  chatgpt chatgpt2  KeyBert  \n",
              "0      0    NaN      0        0      NaN        0  \n",
              "1      0    NaN      0        0      NaN        0  \n",
              "2      0    NaN      0        0      NaN        0  \n",
              "3      0    NaN      0        0      NaN        0  \n",
              "4      0    NaN      0        0      NaN        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c239450-ff58-4817-a66e-70353c6694be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_num</th>\n",
              "      <th>job_description</th>\n",
              "      <th>word</th>\n",
              "      <th>hadar</th>\n",
              "      <th>nisan</th>\n",
              "      <th>judge</th>\n",
              "      <th>final</th>\n",
              "      <th>chatgpt</th>\n",
              "      <th>chatgpt2</th>\n",
              "      <th>KeyBert</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The chosen Sr. Software Developer will be part...</td>\n",
              "      <td>chosen</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>The chosen Sr. Software Developer will be part...</td>\n",
              "      <td>sr</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>The chosen Sr. Software Developer will be part...</td>\n",
              "      <td>part</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>The chosen Sr. Software Developer will be part...</td>\n",
              "      <td>larger</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>The chosen Sr. Software Developer will be part...</td>\n",
              "      <td>team</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c239450-ff58-4817-a66e-70353c6694be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2c239450-ff58-4817-a66e-70353c6694be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2c239450-ff58-4817-a66e-70353c6694be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f7ab89ee-ded2-4f9a-a231-75a77a80d0e2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f7ab89ee-ded2-4f9a-a231-75a77a80d0e2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f7ab89ee-ded2-4f9a-a231-75a77a80d0e2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 13693,\n  \"fields\": [\n    {\n      \"column\": \"job_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job_description\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 227,\n        \"samples\": [\n          \"Ref ID: 02760-0010838185 Classification: Software Engineer Compensation: DOE POSITION DESCRIPTION: We are looking for an engineer with WPF expertise who values and uses best software development practices and technology, is a team player, an innovative problem solver and has a strong desire to deliver quality software. KEY RESPONSIBILITIES INCLUDE: -Develop software components in C#/.NET for a Windows-based system, including collaboration in requirements definition, design, coding, and testing -Follow software engineering practices and design principles that are aligned with the global team -Collaborate with other teams to drive alignment on product requirements and resulting development execution plans -Participate in design reviews and provide input to the design recommendations; incorporate security requirements into design; and provide input to data flow\",\n          \"Position: Software Developer Location:\\u00a0Billerica, MA Duration: Long term contract 12+Month **Looking for local candidate** In-person interview mandatory\\u00a0 Description\\u00a0 Ability to debug and troubleshoot software, configuration and database related issues in a complete Enterprise application SQL skills and experience with Microsoft SQL Server Participate with the team in process improvements and product innovation Skills: Object-oriented design and development SQL Server 2012/2014/2016 Microsoft .NET 4.5 C#.NET Unit Testing (NUnit or equivalent) Qualifications: Energetic result-oriented self-starter Able to work in collaborative Agile environment Fully responsible for design, development, testing, debugging, and support of software programs, applications and projects. Is able to conduct standard analysis on others\\u2019 code. Reviews tasks and provides guidance to other team members. Identifies and solves software compatibility and interface problems. Demonstrates solid presentation skills, creativity and overall domain knowledge Formulates and evaluates alternative designs for their assigned tasks. May act as technical leader on software projects and solutions Anticipates problems and knows who to go to when issues arise. Delivers quality work product through the creation and execution of tests. Must have excellent organizational skills and ability to work independently Customer focused team player with ability to lead and motivate others toward continuous improvement and shared success Excellent communication and interpersonal skills, including the ability to converse with engineers and non-engineers of diverse backgrounds Demonstrated effective leadership and team facilitation skills Ability to handle multiple tasks with competing priorities effectively\",\n          \"Minimum Required Skills: .NET, C#, Visual Studio, SQL Server, Web API, MVC, JavaScript, JQuery, HTML, CSS If you are a Senior Software Developer with experience, please read on! What You Will Be Doing -Be responsible for project estimation/scope, architecture, project team efficiency, communication, deployment and QA in a consulting environment. -Be able to work on multiple projects for multiple clients simultaneously (usually 2-3), work with a cross-functional team, and be transparent about time and tasks to help clients understand the progress of their projects. What You Need for this Position At Least 3 Years of experience and knowledge of: - .NET - C# - Visual Studio - SQL Server - Web API - MVC - JavaScript - JQuery - HTML - CSSSo, if you are a Senior Software Developer with experience, please apply today! Applicants must be authorized to work in the U.S.Please apply directly to by clicking 'Click Here to Apply' with your Word resume! Looking forward to receiving your resume and going over the position in more detail with you. - Not a fit for this position? Click the link at the bottom of this email to search all of our open positions. Looking forward to receiving your resume! CyberCoders CyberCoders, Inc is proud to be an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law. Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire. Copyright 1999 - 2019 . CyberCoders, Inc. All rights reserved.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5290,\n        \"samples\": [\n          \"seeking\",\n          \"nasdaq\",\n          \"Degree in computer science\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hadar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nisan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"judge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4414228822921053,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chatgpt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chatgpt2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KeyBert\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Column Lengths and Nulls"
      ],
      "metadata": {
        "id": "Nf9sNCeQW2tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_column_lengths_and_nulls():\n",
        "    \"\"\"\n",
        "    Test to ensure that specific columns have the same length as the data and that there are no null/None values.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.read_excel(FILE_PATH)\n",
        "\n",
        "        # Check the total length of the data\n",
        "        data_length = len(data)\n",
        "\n",
        "        # Define the columns to check\n",
        "        columns_to_check = ['hadar', 'nisan', 'KeyBert', 'chatgpt', 'final']\n",
        "\n",
        "        # Check if all specified columns are in the data\n",
        "        for column in columns_to_check:\n",
        "            assert column in data.columns, f\"Expected '{column}' in the data\"\n",
        "            # Check if the column has the same length as the dataset\n",
        "            assert len(data[column]) == data_length, f\"Column '{column}' length does not match data length\"\n",
        "            # Check for null or None values specifically in the specified columns\n",
        "            assert data[column].isnull().sum() == 0, f\"Column '{column}' contains null/None values\"\n",
        "\n",
        "        print(\"test_column_lengths_and_nulls: PASSED\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"test_column_lengths_and_nulls: FAILED ({e})\")\n",
        "\n",
        "# Run the test\n",
        "test_column_lengths_and_nulls()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOPJlGBpW3oY",
        "outputId": "4076b40c-abe9-4d24-9e31-a2c90d8944b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_column_lengths_and_nulls: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Kappa Cohen Agreement >=95%"
      ],
      "metadata": {
        "id": "dCCjcwHRYBj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cohen_kappa_agreement():\n",
        "    \"\"\"\n",
        "    Test to calculate the Cohen's Kappa agreement between 'nisan' and 'hadar' columns.\n",
        "    Checks if the agreement is below 95%.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the data\n",
        "        data = pd.read_excel(FILE_PATH)\n",
        "\n",
        "        # Ensure the required columns are present\n",
        "        assert 'nisan' in data.columns, \"Expected 'nisan' in the data\"\n",
        "        assert 'hadar' in data.columns, \"Expected 'hadar' in the data\"\n",
        "\n",
        "        # Calculate Cohen's Kappa agreement\n",
        "        kappa_score = cohen_kappa_score(data['nisan'], data['hadar'])\n",
        "\n",
        "        # Check if the kappa score is below 95%\n",
        "        assert kappa_score >= 0.95, f\"Cohen's Kappa agreement is below 95%: {kappa_score:.4f}\"\n",
        "\n",
        "        print(f\"test_cohen_kappa_agreement: PASSED (Kappa Score: {kappa_score:.4f})\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"test_cohen_kappa_agreement: FAILED ({e})\")\n",
        "\n",
        "# Run the test\n",
        "test_cohen_kappa_agreement()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TYM-hMYTdy",
        "outputId": "f7f229d8-d634-48c8-caa4-df13a9b5fe23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_cohen_kappa_agreement: PASSED (Kappa Score: 0.9521)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Tokenization"
      ],
      "metadata": {
        "id": "yxJiFs7uWYOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokenization():\n",
        "    try:\n",
        "        sample_text = \"Hello, world!\"\n",
        "        tokens = tokenizer(sample_text)\n",
        "        assert 'input_ids' in tokens, \"Tokenization output must include input_ids\"\n",
        "        # Handle special tokens that might be added by the tokenizer\n",
        "        decoded_text = tokenizer.decode(tokens['input_ids'], skip_special_tokens=True,padding=True, truncation=True)\n",
        "        assert decoded_text.strip() == sample_text, \"Decoded tokens should match original text\"\n",
        "        print(\"test_tokenization: PASSED\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"test_tokenization: FAILED ({e})\")\n",
        "\n",
        "# Run the test\n",
        "test_tokenization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mylHKWJWWUE",
        "outputId": "c6752547-1812-4de6-e5be-f33aa81b3e71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_tokenization: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Training Pipeline including compute metrics test:"
      ],
      "metadata": {
        "id": "9v_YxfvLWt9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "MODEL_CHECKPOINT = \"distilroberta-base\"\n",
        "NUM_LABELS = 2"
      ],
      "metadata": {
        "id": "7FJraRC11odh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Downloading stopwords\n",
        "def test_stopwords_download():\n",
        "    try:\n",
        "        nltk.download('stopwords')\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        assert len(stop_words) > 0, \"Stopwords should be downloaded and contain words.\"\n",
        "        print(\"test_stopwords_download: PASSED\")\n",
        "    except Exception as e:\n",
        "        print(f\"test_stopwords_download: FAILED ({e})\")\n",
        "\n",
        "# Test 2: Data Preparation\n",
        "def prepare_data(data):\n",
        "    examples = {\"tokens\": [], \"labels\": []}\n",
        "    grouped = data.groupby('job_description')\n",
        "    for job_desc, group in grouped:\n",
        "        words = group['word'].tolist()\n",
        "        labels = group['final'].tolist()\n",
        "        examples[\"tokens\"].append(words)\n",
        "        examples[\"labels\"].append(labels)\n",
        "    return examples"
      ],
      "metadata": {
        "id": "_9CyAWC31rDk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prepare_data(data):\n",
        "    try:\n",
        "        examples = prepare_data(data)\n",
        "        assert \"tokens\" in examples and \"labels\" in examples, \"Expected 'tokens' and 'labels' in examples.\"\n",
        "        assert len(examples[\"tokens\"]) == len(examples[\"labels\"]), \"Tokens and labels should have the same length.\"\n",
        "        print(\"test_prepare_data: PASSED\")\n",
        "    except Exception as e:\n",
        "        print(f\"test_prepare_data: FAILED ({e})\")"
      ],
      "metadata": {
        "id": "LieT60KO1tbO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True, max_length=128)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Ignore special tokens like padding\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])  # Use the label of the first word piece\n",
        "            else:\n",
        "                label_ids.append(-100)  # Ignore subsequent pieces of split words\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "S_DmVUPE1wkD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokenization_and_alignment(examples):\n",
        "    try:\n",
        "        tokenized_inputs = tokenize_and_align_labels(examples)\n",
        "        assert \"input_ids\" in tokenized_inputs and \"labels\" in tokenized_inputs, \"Expected 'input_ids' and 'labels' in tokenized_inputs.\"\n",
        "        assert len(tokenized_inputs[\"input_ids\"]) == len(tokenized_inputs[\"labels\"]), \"Input_ids and labels should have the same length.\"\n",
        "        print(\"test_tokenization_and_alignment: PASSED\")\n",
        "    except Exception as e:\n",
        "        print(f\"test_tokenization_and_alignment: FAILED ({e})\")"
      ],
      "metadata": {
        "id": "ysu7zgvF1yqP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dataset_conversion(train_data, val_data):\n",
        "    try:\n",
        "        train_dataset = Dataset.from_dict(tokenize_and_align_labels(train_data))\n",
        "        val_dataset = Dataset.from_dict(tokenize_and_align_labels(val_data))\n",
        "        assert len(train_dataset) > 0 and len(val_dataset) > 0, \"Training and validation datasets should not be empty.\"\n",
        "        print(\"test_dataset_conversion: PASSED\")\n",
        "    except Exception as e:\n",
        "        print(f\"test_dataset_conversion: FAILED ({e})\")"
      ],
      "metadata": {
        "id": "k1fO9Q-p11WC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_initialization(model_checkpoint, num_labels):\n",
        "    try:\n",
        "        model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "        print(f\"Loaded model type: {type(model)}\")  # Debugging: Print the type of the model\n",
        "\n",
        "        # Check if the model is a subclass of PreTrainedModel\n",
        "        assert issubclass(type(model), PreTrainedModel), f\"Model should be a subclass of PreTrainedModel, but got {type(model)}.\"\n",
        "\n",
        "        # Ensure the model has the correct number of labels\n",
        "        assert model.config.num_labels == num_labels, f\"Model should have {num_labels} labels, but got {model.config.num_labels}.\"\n",
        "\n",
        "        print(\"test_model_initialization: PASSED\")\n",
        "    except Exception as e:\n",
        "        print(f\"test_model_initialization: FAILED ({e})\")"
      ],
      "metadata": {
        "id": "-_fXxGBK14mD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_training_pipeline():\n",
        "    try:\n",
        "        # Load the dataset\n",
        "        data = pd.read_excel(FILE_PATH)\n",
        "        examples = prepare_data(data)\n",
        "\n",
        "        # Split into training and validation datasets\n",
        "        train_examples, val_examples = train_test_split(\n",
        "            list(zip(examples[\"tokens\"], examples[\"labels\"])), test_size=0.2, random_state=42\n",
        "        )\n",
        "        train_data = {\"tokens\": [x[0] for x in train_examples], \"labels\": [x[1] for x in train_examples]}\n",
        "        val_data = {\"tokens\": [x[0] for x in val_examples], \"labels\": [x[1] for x in val_examples]}\n",
        "\n",
        "        # Convert to Hugging Face Dataset format\n",
        "        train_dataset = Dataset.from_dict(tokenize_and_align_labels(train_data))\n",
        "        val_dataset = Dataset.from_dict(tokenize_and_align_labels(val_data))\n",
        "\n",
        "        # Initialize model and tokenizer\n",
        "        model = AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=NUM_LABELS)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, add_prefix_space=True)\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./results\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=1,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir=\"./logs\",\n",
        "            logging_steps=10,\n",
        "            save_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "        )\n",
        "\n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "        # Trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=lambda p: {'accuracy': 1.0}  # Mock metric for testing\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        eval_results = trainer.evaluate()\n",
        "        assert 'eval_accuracy' in eval_results, \"Evaluation results should contain 'eval_accuracy'\"\n",
        "        print(\"test_training_pipeline: PASSED\")\n",
        "    except Exception as e:\n",
        "        print(f\"test_training_pipeline: FAILED ({e})\")"
      ],
      "metadata": {
        "id": "0vexjifaxxLu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_stopwords_download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFPlboK_2KAu",
        "outputId": "db48f365-a04d-44df-e132-b28887fc8289"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_stopwords_download: PASSED\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(FILE_PATH)\n",
        "test_prepare_data(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9eBiHEw2L2J",
        "outputId": "d6b9e3aa-97a3-42de-bbe9-5c5853b79125"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_prepare_data: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = prepare_data(data)"
      ],
      "metadata": {
        "id": "dxQEA2lE2Nv5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokenization_and_alignment(examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBoE_UxD2Vkg",
        "outputId": "c04bb484-3763-4118-e035-9ddaae4c8d20"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_tokenization_and_alignment: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples, val_examples = train_test_split(\n",
        "    list(zip(examples[\"tokens\"], examples[\"labels\"])), test_size=0.2, random_state=42\n",
        ")\n",
        "train_data = {\"tokens\": [x[0] for x in train_examples], \"labels\": [x[1] for x in train_examples]}\n",
        "val_data = {\"tokens\": [x[0] for x in val_examples], \"labels\": [x[1] for x in val_examples]}"
      ],
      "metadata": {
        "id": "mE-w_QVd2ZWS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_conversion(train_data, val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRtPA2C42crN",
        "outputId": "a0903530-d6a9-4c6f-d3b5-dca126110825"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_dataset_conversion: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_initialization(MODEL_CHECKPOINT, NUM_LABELS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbTg_BMa2d3f",
        "outputId": "a33e9e30-e520-4e83-f42b-eece73e710f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model type: <class 'transformers.models.roberta.modeling_roberta.RobertaForTokenClassification'>\n",
            "test_model_initialization: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_training_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "jlAJP7nn2e8p",
        "outputId": "69e9612d-87e2-4acc-98c2-6466c7f60dce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [91/91 04:55, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.028400</td>\n",
              "      <td>0.143993</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23/23 00:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_training_pipeline: PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: Model Loading from Google Drive"
      ],
      "metadata": {
        "id": "245FPjgUWxfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_loading_from_gdrive():\n",
        "    try:\n",
        "        loaded_model = AutoModelForTokenClassification.from_pretrained(DRIVE_SAVE_DIRECTORY)\n",
        "        loaded_tokenizer = AutoTokenizer.from_pretrained(DRIVE_SAVE_DIRECTORY)\n",
        "        assert loaded_model is not None, \"Model should be loaded successfully\"\n",
        "        assert loaded_tokenizer is not None, \"Tokenizer should be loaded successfully\"\n",
        "        print(\"test_model_loading_from_gdrive: PASSED\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"test_model_loading_from_gdrive: FAILED ({e})\")\n",
        "\n",
        "# Run the test\n",
        "test_model_loading_from_gdrive()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HES3vstqWx5H",
        "outputId": "53754aea-95d7-4044-a7f4-1eafe28b597b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_model_loading_from_gdrive: PASSED\n"
          ]
        }
      ]
    }
  ]
}