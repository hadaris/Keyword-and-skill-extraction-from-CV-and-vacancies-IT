{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhc4h+Oia52zupuRLdeQXw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HadarMiriamIsaacson/BS-SE-24-207/blob/main/How_to_run_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enter the following link and save the folder in it on your gdrive to run it locally:\n",
        " https://drive.google.com/drive/folders/1s7ffc20Z2Ec6X4haU-MNOAsa87Ts7sle\n",
        " save the folder path string (path_to_local_model_on_my_gdrive). run the code provided below. you may preform changes only on the \"text\" variable content."
      ],
      "metadata": {
        "id": "KGydf7wAtN-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaForTokenClassification, RobertaTokenizer\n",
        "\n",
        "# Load the tokenizer and model from the saved path\n",
        "path_to_model_folder = 'path_to_local_model_on_my_gdrive'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(path_to_model_folder)\n",
        "model = RobertaForTokenClassification.from_pretrained(path_to_model_folder)\n",
        "\n",
        "# Move the model to the appropriate device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Example text data to extract keywords from\n",
        "text = \"\"\"\n",
        "C, C++, C#, Qradar, Data analysis, SQL, Hunting cyber activity, Help desk,\n",
        "Java Script, NetWitness, Active Directory, Java, McAfee ePolicy Server, Troubleshooting,\n",
        "Python, Elastic search, Intsights, Machine Learning, Linux, MongoDB.\n",
        "analyzed malicious files, techniques, and vulnerabilities to develop protections for Check Point products.\n",
        "Released daily protection updates and continuously monitored threat detections.\n",
        "Analyzed high-profile cyber-attacks and developed mitigation strategies.\n",
        "Monitored sensor data to identify and manage potential targeted attacks.\n",
        "Handled alerts from field reps and customers, providing immediate threat coverage.\n",
        "Utilized Linux systems to implement and manage security protections.\n",
        "Crane Operator at XYZ Construction\n",
        "Operate and maintain various types of cranes including tower, mobile, and overhead cranes\n",
        "Ensure safe and efficient crane operations on construction sites\n",
        "Collaborate with construction team to ensure timely completion of projects\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "# Run inference to get predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs).logits\n",
        "\n",
        "# Get the predictions by taking the argmax\n",
        "predictions = torch.argmax(outputs, dim=2)\n",
        "\n",
        "# Map predictions to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "predicted_labels = predictions[0].cpu().tolist()\n",
        "\n",
        "# Extract and clean up the extracted keywords\n",
        "keywords = []\n",
        "current_phrase = []\n",
        "for token, label in zip(tokens, predicted_labels):\n",
        "    if label == 1:  # Assuming 1 indicates a keyword\n",
        "        clean_token = token.replace('Ä ', '').strip()  # Remove leading space indicator and trim spaces\n",
        "        if clean_token and clean_token not in ['<s>', '</s>']:\n",
        "            current_phrase.append(clean_token)\n",
        "    else:\n",
        "        if current_phrase:\n",
        "            # Join tokens to form the complete keyword phrase\n",
        "            keywords.append(' '.join(current_phrase))\n",
        "            current_phrase = []\n",
        "\n",
        "# Append the last phrase if it was part of a keyword\n",
        "if current_phrase:\n",
        "    keywords.append(' '.join(current_phrase))\n",
        "\n",
        "# Further clean up keywords by splitting overly long strings and handling punctuation\n",
        "final_keywords = []\n",
        "for kw in keywords:\n",
        "    # Split by common delimiters and add each as a separate keyword\n",
        "    for phrase in kw.split(','):\n",
        "        phrase = phrase.strip()\n",
        "        if phrase and phrase.isascii():\n",
        "            final_keywords.append(phrase)\n",
        "\n",
        "# Print the cleaned keywords\n",
        "print(\"Extracted Keywords:\", final_keywords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jDy1HKOv1I6",
        "outputId": "64baaa2e-5104-4a0b-b079-0788214c38ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Keywords: ['C ++', 'C #', 'Q rad ar', 'Data analysis', 'SQL', 'Hunting cyber activity', 'Help desk', 'Net Witness', 'Active Directory', 'Java', 'Mc Afee e Policy Server', 'Trou bles hooting', 'Elastic search', 'Int s ights', 'Machine Learning', 'Linux', 'techniques', 'mobile']\n"
          ]
        }
      ]
    }
  ]
}